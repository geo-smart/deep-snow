{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7d0c777-b05b-4da8-a9d5-8976a2b11599",
   "metadata": {},
   "source": [
    "# subset training data\n",
    "This notebook contains functions to generate subsets from multiple raster sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fdce8e6-4c17-4933-928d-ca681584adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rasterio as rio\n",
    "import rioxarray as rxr\n",
    "import geopandas as gpd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from rioxarray import merge\n",
    "import gc\n",
    "from pyproj import Proj, transform\n",
    "import math\n",
    "import xdem\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5ede579-055f-403e-8a30-4261ed912d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_all_data(aso_path, home_path):\n",
    "    aso_fn = aso_path.split('/')[-1][:-4]\n",
    "    S1_snowon_path = glob(f'{home_path}/data/S1_rtc/S1_snow-on_*_for_{aso_fn}.nc')[0]\n",
    "    S1_snowoff_path = glob(f'{home_path}/data/S1_rtc/S1_snow-off_*_for_{aso_fn}.nc')[0]\n",
    "    S1_snowon_mean_path = glob(f'{home_path}/data/S1_rtc_mean/S1_snow-on_*_for_{aso_fn}.nc')[0]\n",
    "    S1_snowoff_mean_path = glob(f'{home_path}/data/S1_rtc_mean/S1_snow-off_*_for_{aso_fn}.nc')[0]\n",
    "    S2_path = glob(f'{home_path}/data/S2/S2_*_for_{aso_fn}.nc')[0]\n",
    "    fcf_path = glob(f'{home_path}/data/fcf/fcf_for_{aso_fn}.nc')[0]\n",
    "    dem_path = glob(f'{home_path}/data/cop30/cop30_for_{aso_fn}.nc')[0]\n",
    "    snodas_path = glob(f'{home_path}/data/snodas/snodas_for_{aso_fn}.nc')[0]\n",
    "    \n",
    "    aso_ds = xr.open_dataset(aso_path).squeeze()\n",
    "    aso_ds = aso_ds.rename({'band_data': 'aso_sd'})\n",
    "    aso_ds['aso_sd'] = aso_ds['aso_sd'].where(aso_ds['aso_sd'] >= 0)\n",
    "    \n",
    "    S1_snowon_ds = xr.open_dataset(S1_snowon_path).squeeze()\n",
    "    S1_snowon_ds = S1_snowon_ds.rename({'vv': 'snowon_vv', 'vh':'snowon_vh'})\n",
    "    S1_snowon_ds = S1_snowon_ds.rio.reproject_match(aso_ds, resampling=rio.enums.Resampling.bilinear, crs=aso_ds.rio.crs)\n",
    "    S1_snowoff_ds = xr.open_dataset(S1_snowoff_path).squeeze()\n",
    "    S1_snowoff_ds = S1_snowoff_ds.rename({'vv': 'snowoff_vv', 'vh':'snowoff_vh'})\n",
    "    S1_snowoff_ds = S1_snowoff_ds.rio.reproject_match(aso_ds, resampling=rio.enums.Resampling.bilinear, crs=aso_ds.rio.crs)\n",
    "    \n",
    "    S1_snowon_mean_ds = xr.open_dataset(S1_snowon_mean_path).squeeze()\n",
    "    S1_snowon_mean_ds = S1_snowon_mean_ds.rename({'vv': 'snowon_vv_mean', 'vh':'snowon_vh_mean'})\n",
    "    S1_snowon_mean_ds = S1_snowon_mean_ds.rio.reproject_match(aso_ds, resampling=rio.enums.Resampling.bilinear, crs=aso_ds.rio.crs)\n",
    "    S1_snowoff_mean_ds = xr.open_dataset(S1_snowoff_mean_path).squeeze()\n",
    "    S1_snowoff_mean_ds = S1_snowoff_mean_ds.rename({'vv': 'snowoff_vv_mean', 'vh':'snowoff_vh_mean'})\n",
    "    S1_snowoff_mean_ds = S1_snowoff_mean_ds.rio.reproject_match(aso_ds, resampling=rio.enums.Resampling.bilinear, crs=aso_ds.rio.crs)\n",
    "    \n",
    "    S2_ds = xr.open_dataset(S2_path).squeeze()\n",
    "    S2_ds = S2_ds.rio.write_crs(aso_ds.rio.crs)\n",
    "    S2_ds = S2_ds.rio.reproject_match(aso_ds, resampling=rio.enums.Resampling.bilinear, crs=aso_ds.rio.crs) \n",
    "    \n",
    "    fcf_ds = xr.open_dataset(fcf_path).squeeze()\n",
    "    fcf_ds = fcf_ds.rename({'__xarray_dataarray_variable__': 'fcf'})\n",
    "    fcf_ds = fcf_ds.rio.reproject_match(aso_ds, resampling=rio.enums.Resampling.bilinear, crs=aso_ds.rio.crs)\n",
    "    \n",
    "    dem_ds = xr.open_dataset(dem_path).squeeze()\n",
    "    dem_ds = dem_ds.rio.write_crs(aso_ds.rio.crs)\n",
    "    dem_ds = dem_ds.rename({'__xarray_dataarray_variable__': 'elevation'})\n",
    "    dem_ds = dem_ds.rio.reproject_match(aso_ds, resampling=rio.enums.Resampling.bilinear, crs=aso_ds.rio.crs)\n",
    "\n",
    "    snodas_ds = xr.open_dataset(snodas_path).squeeze()\n",
    "    snodas_ds = snodas_ds.rio.write_crs(aso_ds.rio.crs)\n",
    "    snodas_ds = snodas_ds.rename({'__xarray_dataarray_variable__': 'snodas_sd'})\n",
    "    snodas_ds = snodas_ds.rio.reproject_match(aso_ds, resampling=rio.enums.Resampling.bilinear, crs=aso_ds.rio.crs)\n",
    "    \n",
    "    ds_list = [aso_ds, S1_snowon_ds, S1_snowoff_ds, S1_snowon_mean_ds, S1_snowoff_mean_ds, S2_ds, fcf_ds, dem_ds, snodas_ds]\n",
    "    \n",
    "    ds = xr.merge(ds_list, compat='override', join='override').squeeze()\n",
    "\n",
    "    # add terrain variables\n",
    "    transform = (50, 0.0, ds.isel(x=0, y=0).x.item(), 0.0, 50, ds.isel(x=0, y=0).y.item())\n",
    "    dem = xdem.DEM.from_array(ds.elevation.values, transform, crs=ds.rio.crs)\n",
    "    ds['aspect'] = (('y', 'x'), xdem.terrain.aspect(dem).data.data)\n",
    "    ds['slope'] = (('y', 'x'), xdem.terrain.slope(dem).data.data)\n",
    "    ds['curvature'] = (('y', 'x'), xdem.terrain.curvature(dem).data.data)\n",
    "    ds['tpi'] = (('y', 'x'), xdem.terrain.topographic_position_index(dem).data.data)\n",
    "    ds['tri'] = (('y', 'x'), xdem.terrain.terrain_ruggedness_index(dem).data.data)\n",
    "\n",
    "    return aso_fn, ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd056de-2e7f-4412-864c-afa30e4624a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on utm10n\n",
      "working on train_utm10n_32km.shp\n",
      "working on ASO_50M_SD_American_20230131_clean\n",
      "total subsets from ASO_50M_SD_American_20230131_clean: 100\n",
      "working on ASO_50M_SD_American_20230413_clean\n",
      "total subsets from ASO_50M_SD_American_20230413_clean: 100\n",
      "working on ASO_50M_SD_American_20230428_clean\n",
      "total subsets from ASO_50M_SD_American_20230428_clean: 87\n",
      "working on ASO_50M_SD_American_20230602_clean\n",
      "total subsets from ASO_50M_SD_American_20230602_clean: 100\n",
      "working on ASO_50M_SD_Feather_20220310_clean\n",
      "total subsets from ASO_50M_SD_Feather_20220310_clean: 185\n",
      "working on ASO_50M_SD_Feather_20220331_clean\n",
      "total subsets from ASO_50M_SD_Feather_20220331_clean: 185\n",
      "working on ASO_50M_SD_Feather_20220429_clean\n",
      "total subsets from ASO_50M_SD_Feather_20220429_clean: 185\n",
      "working on ASO_50M_SD_Feather_20230206_clean\n",
      "total subsets from ASO_50M_SD_Feather_20230206_clean: 185\n",
      "working on ASO_50M_SD_Feather_20230409_clean\n",
      "total subsets from ASO_50M_SD_Feather_20230409_clean: 185\n",
      "working on ASO_50M_SD_Feather_20230428_clean\n",
      "total subsets from ASO_50M_SD_Feather_20230428_clean: 179\n",
      "working on ASO_50M_SD_Feather_20230618_clean\n"
     ]
    }
   ],
   "source": [
    "subset_size = 128\n",
    "\n",
    "home_path = '../..'\n",
    "utm_zones = ['utm10n', 'utm11n', 'utm12n', 'utm13n']\n",
    "total_subsets = 0\n",
    "\n",
    "# loop through utm zones\n",
    "for utm_zone in utm_zones:\n",
    "    print(f'working on {utm_zone}')\n",
    "    aso_paths = glob(f'{home_path}/data/ASO/ASO_50m_SD_withS1overpass/{utm_zone}/*')\n",
    "    tile_names = [f'train_{utm_zone}_32km.shp', f'test_{utm_zone}_32km.shp', f'val_{utm_zone}_32km.shp']\n",
    "\n",
    "    # define projections for lat and lon data variables\n",
    "    utm_proj = Proj(proj='utm', zone=utm_zone[3:-1], ellps='WGS84')\n",
    "    wgs84_proj = Proj(proj='latlong', datum='WGS84')\n",
    "\n",
    "    #loop through train, val, test tiles\n",
    "    for tile_set in tile_names:\n",
    "        # open tiles \n",
    "        print(f'working on {tile_set}')\n",
    "        tiles = gpd.read_file(f'{home_path}/data/tiles/{tile_set}')\n",
    "\n",
    "        #loop through ASO rasters\n",
    "        for aso_path in aso_paths:\n",
    "            # open aso raster\n",
    "            raster_subsets = 0\n",
    "            try:\n",
    "                aso_fn, ds = open_all_data(aso_path, home_path)\n",
    "            except:\n",
    "                print('encountered error opening dataset, skipping')\n",
    "                continue\n",
    "            print(f'working on {aso_fn}')\n",
    "\n",
    "            # loop through tiles\n",
    "            for tile in tiles.iterrows():\n",
    "                # clip to tile extent\n",
    "                try:\n",
    "                    tile_ds = ds.rio.clip([tile[1].geometry], crs=ds.rio.crs, drop=True)\n",
    "                except: #except if tile does not overlap aso raster\n",
    "                    continue\n",
    "\n",
    "                # pad ds to tile extent\n",
    "                tile_ds = tile_ds.rio.pad_box(miny=tile[1].geometry.bounds[1],\n",
    "                                              minx=tile[1].geometry.bounds[0],\n",
    "                                              maxy=tile[1].geometry.bounds[3],\n",
    "                                              maxx=tile[1].geometry.bounds[2])\n",
    "\n",
    "                subset_count = 0\n",
    "                #loop through subset locations\n",
    "                for i in range(round((len(tile_ds.x)/subset_size))):\n",
    "                    for j in range(round((len(tile_ds.y)/subset_size))):\n",
    "                        ymin = j*subset_size\n",
    "                        ymax = ymin+subset_size\n",
    "                        xmin = i*subset_size\n",
    "                        xmax = xmin+subset_size\n",
    "            \n",
    "                        subset_ds = tile_ds.isel(x=slice(xmin, xmax), y=slice(ymin, ymax))\n",
    "            \n",
    "                        valid_aso = np.invert(np.isnan(subset_ds.aso_sd))\n",
    "                        \n",
    "                        # check if subset has valid ASO pixels\n",
    "                        if valid_aso.sum() == 0:\n",
    "                            continue\n",
    "\n",
    "                        # make sure dimensions are correct\n",
    "                        if len(subset_ds.x) != subset_size or len(subset_ds.y) != subset_size:\n",
    "                            continue\n",
    "                    \n",
    "                        else: # save subset\n",
    "                            # create map of gaps\n",
    "                            subset_ds['aso_gap_map'] = np.multiply(np.isnan(subset_ds.aso_sd), 1)\n",
    "                            \n",
    "                            # radar gap maps\n",
    "                            subset_ds['rtc_gap_map'] = np.multiply(((np.isnan(subset_ds.snowon_vv) +\n",
    "                                                                     np.isnan(subset_ds.snowon_vh) +\n",
    "                                                                     np.isnan(subset_ds.snowoff_vv) +\n",
    "                                                                     np.isnan(subset_ds.snowoff_vh)) > 0), 1)\n",
    "    \n",
    "                            subset_ds['rtc_mean_gap_map'] = np.multiply(((np.isnan(subset_ds.snowon_vv_mean) +\n",
    "                                                                     np.isnan(subset_ds.snowon_vh_mean) +\n",
    "                                                                     np.isnan(subset_ds.snowoff_vv_mean) +\n",
    "                                                                     np.isnan(subset_ds.snowoff_vh_mean)) > 0), 1)\n",
    "    \n",
    "                            # s2 gap maps\n",
    "                            subset_ds['s2_gap_map'] = np.multiply(((np.isnan(subset_ds.B02) +\n",
    "                                                                   np.isnan(subset_ds.B03) +\n",
    "                                                                   np.isnan(subset_ds.B04) +\n",
    "                                                                   np.isnan(subset_ds.B08) +\n",
    "                                                                   np.isnan(subset_ds.B11)) > 0), 1)\n",
    "\n",
    "                            # add gaps for high probablity cloud cover\n",
    "                            subset_ds['s2_gap_map'] = xr.where(subset_ds['SCL'] == 9, 1, subset_ds['s2_gap_map'])\n",
    "                            subset_ds['s2_gap_map'] = xr.where(subset_ds['SCL'] == 0, 1, subset_ds['s2_gap_map'])\n",
    "                            \n",
    "                            # fill nans with 0 \n",
    "                            subset_ds = subset_ds.fillna(0)\n",
    "\n",
    "                            # add lat and lon variables \n",
    "                            x, y = np.meshgrid(subset_ds['x'].values, subset_ds['y'].values)\n",
    "                            lon, lat = transform(utm_proj, wgs84_proj, x, y)\n",
    "                            subset_ds['latitude'] = (('y', 'x'), lat)\n",
    "                            subset_ds['longitude'] = (('y', 'x'), lon)\n",
    "                            \n",
    "                            subset_count+=1\n",
    "                            total_subsets+=1\n",
    "                            raster_subsets+=1\n",
    "                            \n",
    "                            subset_ds.to_netcdf(f'{home_path}/data/subsets_v4/{tile_set.split(\"_\")[0]}/{aso_fn}_tile{int(tile[1].id)}_s{subset_count}.nc')\n",
    "                            # reproject to wgs for heatmaps\n",
    "                            subset_ds = subset_ds.rio.reproject(\"EPSG:4326\")\n",
    "                            subset_ds.aso_sd.rio.to_raster(f'{home_path}/data/subsets_v4_tif/{tile_set.split(\"_\")[0]}/{aso_fn}_tile{int(tile[1].id)}_s{subset_count}.tif')\n",
    "                        \n",
    "            print(f'total subsets from {aso_fn}: {raster_subsets}')\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "879f9d4f-ec33-409f-b285-4bad46e56e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_ds(ds, subset_size):\n",
    "#     minx = 0\n",
    "#     miny = 0\n",
    "#     maxx = len(ds.x)-subset_size\n",
    "#     maxy = len(ds.y)-subset_size\n",
    "\n",
    "#     sub_minx = random.randint(minx, maxx)\n",
    "#     sub_miny = random.randint(miny, maxy)\n",
    "#     subset = ds.isel(x=slice(sub_minx, sub_minx+subset_size), y=slice(sub_miny, sub_miny+subset_size))\n",
    "    \n",
    "#     return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eb4e962-0a5c-48bd-a3df-590076bef411",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# subset_size=128\n",
    "# # avg_subs_per_pixel=3\n",
    "\n",
    "# home_path = '../..'\n",
    "# utm_zones = ['utm10n', 'utm11n', 'utm12n', 'utm13n']\n",
    "# total_subsets = 0\n",
    "\n",
    "# # loop through utm zones\n",
    "# for utm_zone in utm_zones:\n",
    "#     print(f'working on {utm_zone}')\n",
    "#     aso_paths = glob(f'{home_path}/data/ASO/ASO_50m_SD_withS1overpass/{utm_zone}/*')\n",
    "#     #tile_names = [f'train_aea_25km.shp', f'test_aea_25km.shp', f'val_aea_25km.shp']\n",
    "#     tile_names = [f'train_{utm_zone}_25km.shp', f'test_{utm_zone}_25km.shp', f'val_{utm_zone}_25km.shp']\n",
    "\n",
    "#     # define projections for lat and lon data variables\n",
    "#     utm_proj = Proj(proj='utm', zone=utm_zone[3:-1], ellps='WGS84')\n",
    "#     wgs84_proj = Proj(proj='latlong', datum='WGS84')\n",
    "\n",
    "#     #loop through train, val, test tiles\n",
    "#     for tile_set in tile_names:\n",
    "#         # open tiles \n",
    "#         print(f'working on {tile_set}')\n",
    "#         tiles = gpd.read_file(f'{home_path}/data/polygons/{tile_set}')\n",
    "\n",
    "#         #loop through ASO rasters\n",
    "#         for aso_path in aso_paths:\n",
    "#             # open aso raster\n",
    "#             raster_subsets = 0\n",
    "#             try:\n",
    "#                 aso_fn, ds = open_all_data(aso_path, home_path)\n",
    "#             except:\n",
    "#                 print('encountered error opening dataset, skipping')\n",
    "#                 continue\n",
    "#             print(f'working on {aso_fn}')\n",
    "            \n",
    "#             # reproject tile to utm zone (only needed for aea tiles)\n",
    "#             #tiles = tiles.to_crs(ds.rio.crs)\n",
    "\n",
    "#             # loop through tiles\n",
    "#             for tile in tiles.iterrows():\n",
    "#                 # clip to tile extent\n",
    "#                 try:\n",
    "#                     tile_ds = ds.rio.clip([tile[1].geometry], crs=ds.rio.crs, drop=True)\n",
    "#                 except: #except if tile does not overlap aso raster\n",
    "#                     continue\n",
    "#                 # set number of subsets to grab based on valid pixel count in tile \n",
    "#                 tile_pixel_count = np.invert(np.isnan(tile_ds.aso_sd.values)).sum()\n",
    "#                 tile_coverage_target = 0.95\n",
    "#                 tile_pixel_coverage_target = round(tile_coverage_target*tile_pixel_count)\n",
    "#                 # subset_goal = round(tile_pixel_count/(subset_size**2)*avg_subs_per_pixel)\n",
    "\n",
    "#                 # pad ds to tile extent\n",
    "#                 tile_ds = tile_ds.rio.pad_box(miny=tile[1].geometry.bounds[1],\n",
    "#                                               minx=tile[1].geometry.bounds[0],\n",
    "#                                               maxy=tile[1].geometry.bounds[3],\n",
    "#                                               maxx=tile[1].geometry.bounds[2])\n",
    "\n",
    "#                 # initialize coverage array\n",
    "#                 tile_ds['selected_data'] = (('y', 'x'), np.full((len(tile_ds.y), len(tile_ds.x)), False))\n",
    "#                 tile_ds['count_map'] = (('y', 'x'), np.full((len(tile_ds.y), len(tile_ds.x)), 0))\n",
    "\n",
    "#                 subset_count = 0\n",
    "#                 tile_pixel_coverage = 0\n",
    "#                 while tile_pixel_coverage < tile_pixel_coverage_target:\n",
    "#                     subset_ds = sample_ds(tile_ds, subset_size)\n",
    "#                     valid_aso = np.invert(np.isnan(subset_ds.aso_sd))\n",
    "                    \n",
    "#                     # check if subset has valid ASO pixels\n",
    "#                     if valid_aso.sum() == 0:\n",
    "#                         continue\n",
    "\n",
    "#                     # check if subset contains ASO data not previously sampled\n",
    "#                     new_data = valid_aso & ~tile_ds.selected_data.sel(x=slice(subset_ds.x.min(), subset_ds.x.max()),\n",
    "#                                                                         y=slice(subset_ds.y.max(), subset_ds.y.min()))\n",
    "#                     if new_data.sum() == 0:\n",
    "#                         continue\n",
    "                    \n",
    "#                     else: # save subset\n",
    "#                         # create map of gaps\n",
    "#                         subset_ds['aso_gap_map'] = np.multiply(np.isnan(subset_ds.aso_sd), 1)\n",
    "                        \n",
    "#                         # radar gap maps\n",
    "#                         subset_ds['rtc_gap_map'] = np.multiply(((np.isnan(subset_ds.snowon_vv) +\n",
    "#                                                                  np.isnan(subset_ds.snowon_vh) +\n",
    "#                                                                  np.isnan(subset_ds.snowoff_vv) +\n",
    "#                                                                  np.isnan(subset_ds.snowoff_vh)) > 0), 1)\n",
    "\n",
    "#                         subset_ds['rtc_mean_gap_map'] = np.multiply(((np.isnan(subset_ds.snowon_vv_mean) +\n",
    "#                                                                  np.isnan(subset_ds.snowon_vh_mean) +\n",
    "#                                                                  np.isnan(subset_ds.snowoff_vv_mean) +\n",
    "#                                                                  np.isnan(subset_ds.snowoff_vh_mean)) > 0), 1)\n",
    "\n",
    "#                         # s2 gap maps\n",
    "#                         subset_ds['s2_gap_map'] = np.multiply(np.isnan(subset_ds.B02), 1)\n",
    "\n",
    "#                         # Update the selected array\n",
    "#                         tile_ds['selected_data'].loc[dict(x=slice(subset_ds.x.min(), subset_ds.x.max()),\n",
    "#                                                           y=slice(subset_ds.y.max(), subset_ds.y.min()))] = valid_aso\n",
    "#                         tile_ds['count_map'].loc[dict(x=slice(subset_ds.x.min(), subset_ds.x.max()),\n",
    "#                                                           y=slice(subset_ds.y.max(), subset_ds.y.min()))] += 1\n",
    "#                         tile_pixel_coverage = tile_ds['selected_data'].sum()\n",
    "                        \n",
    "#                         # fill nans with 0 \n",
    "#                         subset_ds = subset_ds.fillna(0)\n",
    "\n",
    "#                         # add lat and lon variables \n",
    "#                         x, y = np.meshgrid(subset_ds['x'].values, subset_ds['y'].values)\n",
    "#                         lon, lat = transform(utm_proj, wgs84_proj, x, y)\n",
    "#                         subset_ds['latitude'] = (('y', 'x'), lat)\n",
    "#                         subset_ds['longitude'] = (('y', 'x'), lon)\n",
    "                        \n",
    "#                         subset_count+=1\n",
    "#                         total_subsets+=1\n",
    "#                         raster_subsets+=1\n",
    "                        \n",
    "#                         subset_ds.to_netcdf(f'{home_path}/data/subsets_v2/{tile_set.split(\"_\")[0]}/{aso_fn}_tile{int(tile[1].id)}_s{subset_count}.nc')\n",
    "#                         # reproject to wgs for heatmaps\n",
    "#                         subset_ds = subset_ds.rio.reproject(\"EPSG:4326\")\n",
    "#                         subset_ds.aso_sd.rio.to_raster(f'{home_path}/data/subsets_v2_tif/{tile_set.split(\"_\")[0]}/{aso_fn}_tile{int(tile[1].id)}_s{subset_count}.tif')\n",
    "\n",
    "#                 # remove redundant subsets\n",
    "#                 subset_fns = glob(f'{home_path}/data/subsets_v2/{tile_set.split(\"_\")[0]}/{aso_fn}_tile{int(tile[1].id)}_*.nc')\n",
    "#                 removed_count = 0\n",
    "#                 for fn in subset_fns:\n",
    "#                     subset_ds = xr.open_dataset(fn)\n",
    "#                     count_map_min = tile_ds['count_map'].sel(x=slice(subset_ds.x.min(), subset_ds.x.max()), y=slice(subset_ds.y.max(), subset_ds.y.min())).min()\n",
    "#                     if count_map_min >= 2:\n",
    "#                         !rm $fn\n",
    "#                         tif_fn = f'{home_path}/data/subsets_v2_tif/{tile_set.split(\"_\")[0]}/{fn.split(\"/\")[-1][:-3]}.tif'\n",
    "#                         !rm $tif_fn\n",
    "#                         tile_ds['count_map'].loc[dict(x=slice(subset_ds.x.min(), subset_ds.x.max()),\n",
    "#                                                       y=slice(subset_ds.y.max(), subset_ds.y.min()))] -= 1\n",
    "#                         removed_count += 1\n",
    "#                         raster_subsets -=1\n",
    "                        \n",
    "#             print(f'total subsets from {aso_fn}: {raster_subsets}')\n",
    "#             gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gda]",
   "language": "python",
   "name": "conda-env-gda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
