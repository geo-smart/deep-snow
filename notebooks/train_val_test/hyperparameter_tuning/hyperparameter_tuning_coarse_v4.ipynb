{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce8b6311-386c-44e8-b59f-be171ef2de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "import deep_snow.models\n",
    "import deep_snow.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae392541-49b6-44e4-876a-8cb2ac6c651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def set_seed(seed: int = 43):\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#     set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2273904e-09b0-4d19-ac10-ce094a3246e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_lognormal(center, sigma):\n",
    "    mu = np.log(center)\n",
    "    return np.random.lognormal(mean=mu, sigma=sigma)\n",
    "\n",
    "# def visualize_lognormal(center, sigmas, n_samples=10000):\n",
    "#     f, ax = plt.subplots(figsize=(10, 6))\n",
    "#     for sigma in sigmas:\n",
    "#         mu = np.log(center)\n",
    "#         samples = np.random.lognormal(mean=mu, sigma=sigma, size=n_samples)\n",
    "#         sns.kdeplot(samples, label=f'sigma={sigma}', ax=ax)\n",
    "#     ax.set_xlabel('Sampled Value')\n",
    "#     ax.set_ylabel('Density')\n",
    "#     ax.set_title(f'Lognormal Samples Centered on {center}')\n",
    "#     f.legend()\n",
    "#     ax.grid(True, which='both', linestyle='--')\n",
    "#     ax.set_xlim(0, 0.0005)\n",
    "#     ax.set_ylim(0)\n",
    "\n",
    "# # Example usage\n",
    "# visualize_lognormal(center=1e-4, sigmas=[1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ebfb9b4-182e-4b0b-ad88-5c73f6572634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get paths to data\n",
    "train_data_dir = '/mnt/working/brencher/repos/deep-snow/data/subsets_v4/train'\n",
    "train_path_list = glob(f'{train_data_dir}/ASO_50M_SD*.nc')\n",
    "\n",
    "val_data_dir = '/mnt/working/brencher/repos/deep-snow/data/subsets_v4/val'\n",
    "val_path_list = glob(f'{val_data_dir}/ASO_50M_SD*.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d359e81-aa1a-4b3b-b496-2e1e77c029f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input_channels, return_channels, epochs, lr, weight_decay, n_layers=5):\n",
    "    model = deep_snow.models.ResDepth(n_input_channels=len(input_channels), depth=n_layers)\n",
    "    model_name = f'ResDepth_lr{lr}_weightdecay{weight_decay}'\n",
    "    model.to('cuda');  # Run on GPU\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "    loss_fn = nn.MSELoss()\n",
    "    epochs = epochs\n",
    "    \n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    counter = 0\n",
    "    min_val_loss = 1\n",
    "    patience = 0\n",
    "    patience_limit = 30\n",
    "\n",
    "    # training and validation loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f'\\nStarting epoch {epoch+1}')\n",
    "        train_epoch_loss = []\n",
    "        val_epoch_loss = []\n",
    "            \n",
    "        # Loop through training data with tqdm progress bar\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\", ncols=130)\n",
    "        for data_tuple in pbar:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # read data into dictionary\n",
    "            data_dict = {name: tensor for name, tensor in zip(return_channels, data_tuple)}\n",
    "            # prepare inputs by concatenating along channel dimension\n",
    "            inputs = torch.cat([data_dict[channel] for channel in input_channels], dim=1).to('cuda')\n",
    "    \n",
    "            # generate prediction\n",
    "            pred_sd = model(inputs)\n",
    "    \n",
    "            # Limit prediction to areas with valid data\n",
    "            pred_sd = torch.where(data_dict['aso_gap_map'].to('cuda') + data_dict['rtc_gap_map'].to('cuda') + data_dict['s2_gap_map'].to('cuda') == 0, pred_sd, torch.zeros_like(pred_sd).to('cuda'))\n",
    "            aso_sd = torch.where(data_dict['aso_gap_map'].to('cuda') + data_dict['rtc_gap_map'].to('cuda') + data_dict['s2_gap_map'].to('cuda') == 0, data_dict['aso_sd'].to('cuda'), torch.zeros_like(pred_sd).to('cuda'))\n",
    "    \n",
    "            # Calculate loss\n",
    "            train_batch_loss = loss_fn(pred_sd, aso_sd.to('cuda'))\n",
    "            train_epoch_loss.append(train_batch_loss.item())\n",
    "    \n",
    "            # Update tqdm progress bar with batch loss\n",
    "            pbar.set_postfix({'batch loss': train_batch_loss.item(), 'mean epoch loss': np.mean(train_epoch_loss)})\n",
    "    \n",
    "            train_batch_loss.backward()  # Propagate the gradients in backward pass\n",
    "            optimizer.step()\n",
    "    \n",
    "        train_loss.append(np.mean(train_epoch_loss))\n",
    "        print(f'Training loss: {np.mean(train_epoch_loss)}')\n",
    "    \n",
    "        # Run model on validation data with tqdm progress bar\n",
    "        for data_tuple in tqdm(val_loader, desc=\"Validation\", unit=\"batch\"):\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                \n",
    "                # read data into dictionary\n",
    "                data_dict = {name: tensor for name, tensor in zip(return_channels, data_tuple)}\n",
    "                # prepare inputs by concatenating along channel dimension\n",
    "                inputs = torch.cat([data_dict[channel] for channel in input_channels], dim=1).to('cuda')\n",
    "        \n",
    "                # generate prediction\n",
    "                pred_sd = model(inputs)\n",
    "        \n",
    "                # Limit prediction to areas with valid data\n",
    "                pred_sd = torch.where(data_dict['aso_gap_map'].to('cuda') + data_dict['rtc_gap_map'].to('cuda') + data_dict['s2_gap_map'].to('cuda') == 0, pred_sd, torch.zeros_like(pred_sd).to('cuda'))\n",
    "                aso_sd = torch.where(data_dict['aso_gap_map'].to('cuda') + data_dict['rtc_gap_map'].to('cuda') + data_dict['s2_gap_map'].to('cuda') == 0, data_dict['aso_sd'].to('cuda'), torch.zeros_like(pred_sd).to('cuda'))\n",
    "        \n",
    "                # Calculate loss\n",
    "                val_batch_loss = loss_fn(pred_sd, aso_sd.to('cuda'))\n",
    "                val_epoch_loss.append(val_batch_loss.item())\n",
    "    \n",
    "        val_loss.append(np.mean(val_epoch_loss))\n",
    "        print(f'Validation loss: {np.mean(val_epoch_loss)}')\n",
    "        scheduler.step(np.mean(val_epoch_loss))\n",
    "\n",
    "        # save loss \n",
    "        with open(f'../../../loss/{model_name}_val_loss.pkl', 'wb') as f:\n",
    "            pickle.dump(val_loss, f)\n",
    "            \n",
    "        with open(f'../../../loss/{model_name}_train_loss.pkl', 'wb') as f:\n",
    "            pickle.dump(train_loss, f)\n",
    "        \n",
    "        # Early stopping check (start saving after 30 epochs)\n",
    "        if np.mean(val_epoch_loss) < min_val_loss:\n",
    "            min_val_loss = np.mean(val_epoch_loss)\n",
    "            min_val_loss_epoch = epoch\n",
    "            patience = 0\n",
    "            if epoch > 30:\n",
    "                torch.save(model.state_dict(), f'../../../weights/{model_name}_epochs{epoch}_minvalloss{min_val_loss:.5f}')\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        if patience >= patience_limit:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch + 1}. No improvement in validation loss for {patience_limit} epochs.\")\n",
    "            break\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        print(f'epoch time: {epoch_end_time - epoch_start_time:.4f} seconds')\n",
    "\n",
    "    #plot_loss(train_loss, val_loss)\n",
    "    return [min_val_loss_epoch, min_val_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1e7003d-0227-472d-8f2d-25c83677c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data to be returned by dataloader\n",
    "return_channels = [\n",
    "    # ASO products\n",
    "    'aso_sd', # ASO lidar snow depth (target dataset)\n",
    "    'aso_gap_map', # gaps in ASO data\n",
    "    \n",
    "    'delta_cr', # change in cross ratio, snowon_cr - snowoff_cr\n",
    "    'rtc_gap_map', # gaps in Sentinel-1 data\n",
    "   \n",
    "    # Sentinel-2 products \n",
    "    'blue', # snow on Sentinel-2 blue band\n",
    "    'swir1', # snow on Sentinel-2 shortwave infrared band 1\n",
    "    'ndsi', # Normalized Difference Snow Index from Sentinel-2\n",
    "    's2_gap_map', # gaps in Sentinel-2 data\n",
    "\n",
    "    # snodas datset\n",
    "    'snodas_sd', # snow depth\n",
    "\n",
    "    # PROBA-V global land cover dataset (Buchhorn et al., 2020)\n",
    "    'fcf', # fractional forest cover\n",
    "    \n",
    "    # COP30 digital elevation model      \n",
    "    'elevation',\n",
    "    'slope',\n",
    "    'northness',\n",
    "    'curvature',\n",
    "\n",
    "    # day of water year\n",
    "    'dowy'\n",
    "                    ]\n",
    "\n",
    "# prepare training and validation dataloaders\n",
    "train_data = deep_snow.dataset.Datasetv2(train_path_list, return_channels, norm=True, cache_data=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n",
    "val_data = deep_snow.dataset.Datasetv2(val_path_list, return_channels, norm=True, augment=False, cache_data=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0e3566d-65a3-4055-b215-3e8ab17fab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input channels for model\n",
    "input_channels = ['snodas_sd',\n",
    "                  'blue',\n",
    "                  'swir1',\n",
    "                  'ndsi',\n",
    "                  'elevation',\n",
    "                  'northness',\n",
    "                  'slope',\n",
    "                  'curvature',\n",
    "                  'dowy',\n",
    "                  'delta_cr',\n",
    "                  'fcf'\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c41200-9aa1-40f1-a91a-7f16cc838ad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_trials = 20\n",
    "epochs=500\n",
    "exp_dict = {}\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    \n",
    "    print('---------------------------------------------------------')\n",
    "    print(f'starting trial {trial}')\n",
    "    lr = sample_lognormal(center=3e-4, sigma=1.0)\n",
    "    weight_decay = sample_lognormal(center=1e-4, sigma=1.0)\n",
    "    print(f'lr: {lr}, weight decay: {weight_decay}')\n",
    "    min_val_loss_epoch, min_val_loss = train_model(input_channels, return_channels, epochs=epochs, lr=lr, weight_decay=weight_decay)\n",
    "    print(f'lr: {lr}, weight decay: {weight_decay}, final epoch: {min_val_loss_epoch}, final val loss: {min_val_loss}')\n",
    "    exp_dict[trial] = [lr, weight_decay, min_val_loss_epoch, min_val_loss]\n",
    "    # save experiments \n",
    "    with open(f'../../../loss/ResDepth_lr_tuning_loss_v4.pkl', 'wb') as f:\n",
    "        pickle.dump(exp_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-snow] *",
   "language": "python",
   "name": "conda-env-deep-snow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
