{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58e16ec1-3582-49db-b54d-2f9ac20542a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from glob import glob\n",
    "\n",
    "import deep_snow.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a935a02-8340-4c11-b9af-e699332f27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = '/mnt/Backups/gbrench/repos/deep-snow/data/subsets_v4/train'\n",
    "train_path_list = glob(f'{train_data_dir}/ASO_50M_SD*.nc')\n",
    "\n",
    "val_data_dir = '/mnt/Backups/gbrench/repos/deep-snow/data/subsets_v4/val'\n",
    "val_path_list = glob(f'{val_data_dir}/ASO_50M_SD*.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bfc20ee-7773-4430-8d68-e1ca613b4894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data to be returned by dataloader\n",
    "all_channels = [\n",
    "    # ASO products\n",
    "    'aso_sd', # ASO lidar snow depth (target dataset)\n",
    "    'aso_gap_map', # gaps in ASO data\n",
    "    \n",
    "    # Sentinel-1 products\n",
    "    'snowon_vv', # snow on Sentinel-1 VV polarization backscatter in dB, closest acquisition to ASO acquisition\n",
    "    'snowon_vh', # snow on Sentinel-1 VH polarization backscatter in dB, closest acquisition to ASO acquisition\n",
    "    'snowoff_vv', # snow off Sentinel-1 VV polarization backscatter in dB, closest acquisition to ASO acquisition\n",
    "    'snowoff_vh', # snow off Sentinel-1 VH polarization backscatter in dB, closest acquisition to ASO acquisition\n",
    "    'snowon_vv_mean', # snow on Sentinel-1 VV polarization backscatter in dB, mean of acquisition in 4 week period around ASO acquisition\n",
    "    'snowon_vh_mean', # snow on Sentinel-1 VH polarization backscatter in dB, mean of acquisition in 4 week period around ASO acquisition\n",
    "    'snowoff_vv_mean', # snow off Sentinel-1 VV polarization backscatter in dB, mean of acquisition in 4 week period around ASO acquisition\n",
    "    'snowoff_vh_mean', # snow off Sentinel-1 VH polarization backscatter in dB, mean of acquisition in 4 week period around ASO acquisition\n",
    "    'snowon_cr', # cross ratio, snowon_vh - snowon_vv\n",
    "    'snowoff_cr', # cross ratio, snowoff_vh - snowoff_vv\n",
    "    'delta_cr', # change in cross ratio, snowon_cr - snowoff_cr\n",
    "    'rtc_gap_map', # gaps in Sentinel-1 data\n",
    "    'rtc_mean_gap_map', # gaps in Sentinel-1 mean data\n",
    "    \n",
    "    # Sentinel-2 products \n",
    "    'aerosol_optical_thickness', # snow on Sentinel-2 aerosol optical thickness band \n",
    "    'coastal_aerosol', # snow on Sentinel-2 coastal aerosol band\n",
    "    'blue', # snow on Sentinel-2 blue band\n",
    "    'green', # snow on Sentinel-2 green band\n",
    "    'red', # snow on Sentinel-2 red band\n",
    "    'red_edge1', # snow on Sentinel-2 red edge 1 band\n",
    "    'red_edge2', # snow on Sentinel-2 red edge 2 band\n",
    "    'red_edge3', # snow on Sentinel-2 red edge 3 band\n",
    "    'nir', # snow on Sentinel-2 near infrared band\n",
    "    'water_vapor', # snow on Sentinel-2 water vapor\n",
    "    'swir1', # snow on Sentinel-2 shortwave infrared band 1\n",
    "    'swir2', # snow on Sentinel-2 shortwave infrared band 2\n",
    "    'scene_class_map', # snow on Sentinel-2 scene classification product\n",
    "    'water_vapor_product', # snow on Sentinel-2 water vapor product\n",
    "    'ndvi', # Normalized Difference Vegetation Index from Sentinel-2\n",
    "    'ndsi', # Normalized Difference Snow Index from Sentinel-2\n",
    "    'ndwi', # Normalized Difference Water Index from Sentinel-2\n",
    "    's2_gap_map', # gaps in Sentinel-2 data\n",
    "\n",
    "    # SNODAS \n",
    "    'snodas_sd', # snodas snow depth\n",
    "\n",
    "    # PROBA-V global land cover dataset (Buchhorn et al., 2020)\n",
    "    'fcf', # fractional forest cover\n",
    "    \n",
    "    # COP30 digital elevation model      \n",
    "    'elevation',\n",
    "    'slope',\n",
    "    'aspect',\n",
    "    'curvature',\n",
    "    'tpi',\n",
    "    'tri',\n",
    "\n",
    "    # latitude and longitude\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "\n",
    "    # day of water year\n",
    "    'dowy'\n",
    "                    ]\n",
    "\n",
    "# prepare training and validation dataloaders\n",
    "train_data = deep_snow.dataset.Dataset(train_path_list, all_channels, norm=False)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=2000, shuffle=False)\n",
    "\n",
    "# prepare training and validation dataloaders\n",
    "val_data = deep_snow.dataset.Dataset(val_path_list, all_channels, norm=False)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_data, batch_size=2000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd0154b3-fd19-4129-9ebe-4203ce38b2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop 1\n",
      "loop 2\n",
      "loop 3\n",
      "loop 4\n",
      "loop 5\n",
      "loop 6\n",
      "loop 7\n"
     ]
    }
   ],
   "source": [
    "# find dataset min and max for normalization\n",
    "norm_dict = {}\n",
    "for i, outputs in enumerate(train_loader):\n",
    "    print(f'loop {i+1}')\n",
    "    for j, item in enumerate(outputs):\n",
    "        data_name = selected_channels[j]\n",
    "        if i == 0:\n",
    "            norm_dict[data_name] = [item.min().item(), item.max().item()]\n",
    "        if item.max() > norm_dict[data_name][1]:\n",
    "            norm_dict[data_name][1] = item.max().item()\n",
    "        if item.min() < norm_dict[data_name][0] and not item.min() == 0:\n",
    "            norm_dict[data_name][0] = item.min().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1d07fb7-a318-43dd-b96a-a82916bf4a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aso_sd': [0.0, 397.2589111328125],\n",
       " 'aso_gap_map': [0.0, 1.0],\n",
       " 'snowon_vv': [-57.02630615234375, 29.14653778076172],\n",
       " 'snowon_vh': [-64.75164794921875, 15.596705436706543],\n",
       " 'snowoff_vv': [-52.30192184448242, 26.94390869140625],\n",
       " 'snowoff_vh': [-61.0316047668457, 15.569765090942383],\n",
       " 'snowon_vv_mean': [-57.02630615234375, 29.60448455810547],\n",
       " 'snowon_vh_mean': [-64.75164794921875, 16.35344696044922],\n",
       " 'snowoff_vv_mean': [-54.76593780517578, 26.17017364501953],\n",
       " 'snowoff_vh_mean': [-60.62683868408203, 14.829648971557617],\n",
       " 'snowon_cr': [-38.32542419433594, 13.106416702270508],\n",
       " 'snowoff_cr': [-37.60618209838867, 11.941126823425293],\n",
       " 'delta_cr': [-32.43565368652344, 23.977340698242188],\n",
       " 'rtc_gap_map': [0.0, 1.0],\n",
       " 'rtc_mean_gap_map': [0.0, 1.0],\n",
       " 'aerosol_optical_thickness': [0.0, 572.0],\n",
       " 'coastal_aerosol': [0.0, 24304.0],\n",
       " 'blue': [0.0, 23371.0],\n",
       " 'green': [0.0, 26440.0],\n",
       " 'red': [0.0, 21994.0],\n",
       " 'red_edge1': [0.0, 21321.0],\n",
       " 'red_edge2': [0.0, 21131.0],\n",
       " 'red_edge3': [0.0, 20978.0],\n",
       " 'nir': [0.0, 21622.0],\n",
       " 'water_vapor': [0.0, 18199.0],\n",
       " 'swir1': [0.0, 18033.0],\n",
       " 'swir2': [0.0, 18078.0],\n",
       " 'scene_class_map': [0.0, 15.0],\n",
       " 'water_vapor_product': [0.0, 6517.43798828125],\n",
       " 'ndvi': [-1.0, 1.0],\n",
       " 'ndsi': [-1.0, 1.0],\n",
       " 'ndwi': [-1.0, 1.0],\n",
       " 's2_gap_map': [0.0, 1.0],\n",
       " 'fcf': [0.0, 1.0],\n",
       " 'elevation': [0.0, 4408.29150390625],\n",
       " 'slope': [0.0, 81.66542053222656],\n",
       " 'aspect': [0.0, 359.9999694824219],\n",
       " 'curvature': [-21.7579288482666, 21.967069625854492],\n",
       " 'tpi': [-163.7117919921875, 166.3883056640625],\n",
       " 'tri': [0.0, 912.681884765625],\n",
       " 'latitude': [35.72165298461914, 48.1369743347168],\n",
       " 'longitude': [-123.98522186279297, -105.17345428466797],\n",
       " 'dowy': [112.0, 319.0]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7995b63-b4c8-402b-b211-ebd34806ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find dataset 1st and 99th percentiles for normalization\n",
    "train_norm_dict = {}\n",
    "val_norm_dict = {}\n",
    "\n",
    "for channel in all_channels:\n",
    "    # prepare training and validation dataloaders\n",
    "    train_data = deep_snow.dataset.Dataset(train_path_list, [channel], norm=False)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=2000, shuffle=False)\n",
    "    \n",
    "    # prepare training and validation dataloaders\n",
    "    val_data = deep_snow.dataset.Dataset(val_path_list, [channel], norm=False)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_data, batch_size=2000, shuffle=False)\n",
    "    \n",
    "    train_array = np.Array([])\n",
    "    for i, outputs in enumerate(train_loader):\n",
    "        print(f'loop {i+1}')\n",
    "        train_array = np.concatenate(train_array, output.flatten.numpy())\n",
    "    train_array = train_array[train_array != 0]\n",
    "    p1, p99 = np.percentile(train_array, [1, 99])\n",
    "    del train_array\n",
    "    train_norm_dict[channel] = [p1, p99]\n",
    "            \n",
    "    val_array = np.Array([])\n",
    "    for i, outputs in enumerate(val_loader):\n",
    "        print(f'loop {i+1}')\n",
    "        val_array = np.concatenate(val_array, output.flatten.numpy())\n",
    "    val_array = val_array[val_array != 0]\n",
    "    p1, p99 = np.percentile(val_array, [1, 99])\n",
    "    del val_array\n",
    "    val_norm_dict[channel] = [p1, p99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2bbd13-21b1-4de7-acd2-76709071a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_norm_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:crunchy-snow] *",
   "language": "python",
   "name": "conda-env-crunchy-snow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
